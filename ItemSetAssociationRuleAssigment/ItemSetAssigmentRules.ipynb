{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rule Mining with FP-Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### The data use is from UCI Machine Learning Repository.  The name of th data file is mushroom.data. The dataset has 23 features taken from species of gilled mushrooms and contains 8124 observations. Each of the features contains nominal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Import Neccessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Context for the communication between the client and the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Import the data from a file and create an RDD because the algorithm expect a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(\"mushroom.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Parse the data line by line an create the needed strcuture for the algorithme. Each line is splitted by using as delimiter  the blank character (' '). And the map transformation create anothet RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = data.map(lambda line: line.strip().split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To limit the number of ItemSet the support is fixed to 0.9. For optimization resaon 10 partitions are use. The FRGrowth  algorithm is called via the train method of the FPGrowth object which is imporrted at the beginning of the file. The Train  method,as argument, expect an structured RDD, a minimum support and a number of partitions.\n",
    "\n",
    "#####  With the trained data (Transactions data) the model is fitted and generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FPGrowth.train(transactions, minSupport = 0.90, numPartitions = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the model the frquents ItemsSet are extracted by called the action Collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqItemsets = model.freqItemsets().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of itemSets is obtained by called the function len on the frquenset objec above. With the support of 90% the number of  FrequentItemSets is 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freqItemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A Dictionnary is difined in order to tract the frequence of each itemSet in the DataSet (transaction Data). The Key is the ItemSDets and the values is the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqItemsetsDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #####  The freqence Items Set object obtained from the model is used to fill the above dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(freqItemsets)):\n",
    "    freqItemsetsDict[str(freqItemsets[i].items)] = freqItemsets[i].freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  To extract the the associated Associatiion Rule 60% of confidence level is used to minimize the number of rule. It is user difined value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "minConfidence = .6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  From the model and the confidence value the rule are extracted via the collect action. And the rule are sorted by confidencee via the sorted method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = sorted(model._java_model.generateAssociationRules(minConfidence).collect(), key=lambda x: x.confidence(), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Get the numbeer rules by called the len method with argumeent as the rules object generated above. The number of rulesfor a confidence of 60% is 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the rules object extracted above get for each rule the antecedent and the consequent and use  them with the dictionary freqItemsetsDict to \n",
    "calculate the support and the confidence. Use theses value for visualization (print)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['34', '86'] => ['85'] (support: 0.97 confidence: 1.00)\n",
      "['34'] => ['85'] (support: 0.97 confidence: 1.00)\n",
      "['86'] => ['85'] (support: 0.98 confidence: 1.00)\n",
      "['90'] => ['85'] (support: 0.92 confidence: 1.00)\n",
      "['34'] => ['86'] (support: 0.97 confidence: 1.00)\n",
      "['34', '85'] => ['86'] (support: 0.97 confidence: 1.00)\n",
      "['86'] => ['34'] (support: 0.97 confidence: 1.00)\n",
      "['86', '85'] => ['34'] (support: 0.97 confidence: 1.00)\n",
      "['85'] => ['86'] (support: 0.97 confidence: 0.98)\n",
      "['85'] => ['34'] (support: 0.97 confidence: 0.97)\n",
      "['85'] => ['90'] (support: 0.97 confidence: 0.92)\n"
     ]
    }
   ],
   "source": [
    "for j in range(0, len(rules)):\n",
    "    antecedent = list(rules[j].antecedent())\n",
    "    consequent = list(rules[j].consequent())\n",
    "    ruleItemset = str(antecedent + consequent)\n",
    "    \n",
    "    try:\n",
    "        support = \"{0:.2f}\".format(freqItemsetsDict[ruleItemset] / transactions.count())\n",
    "    except KeyError:\n",
    "        next\n",
    "    \n",
    "    confidence = \"{0:.2f}\".format(rules[j].confidence())\n",
    "    \n",
    "    try:\n",
    "        print(antecedent, '=>', consequent, '(support: '+ str(support), 'confidence: '+ str(confidence) + ')')\n",
    "    except NameError:\n",
    "        next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrpretation for the rules aboves:\n",
    "\n",
    "['34', '86'] => ['85'] (support: 0.97 confidence: 1.00) means 97% of the dataSet (transactions contains the item 34, 86 and 85) and each transaction that contains 34 and 86 conatins also 85 with certitude (100%)\n",
    "\n",
    "['90'] => ['85'] (support: 0.92 confidence: 1.00)\n",
    "eans 92% of the dataSet (transactions contains the item 90 and 85) and each transaction that contains 90 conatains also 85 with certitude (100%)\n",
    "\n",
    "['85'] => ['86'] (support: 0.97 confidence: 0.98)\n",
    "eans 97% of the dataSet (transactions contains the item 85 and 86) and each transaction that contains 85 conatins also 85 with a probability of 98%\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
